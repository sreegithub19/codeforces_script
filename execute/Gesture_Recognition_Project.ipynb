{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9A3Yt0sCjeL"
      },
      "source": [
        "# Gesture Recognition\n",
        " - Developers: Sreedhar K and Munirathinam Duraisamy\n",
        "\n",
        " Note:\n",
        " For the submitted assignment (with the .h5 file), we can download from the following link:\n",
        " https://mail.google.com/mail/u/0/?tab=rm&ogbl#search/model5/FMfcgzGrbvFHSjwJZWpRhrkJkpxqwDvb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjB-UtRQCjeM"
      },
      "source": [
        "# Table of contents:\n",
        "\n",
        "- [Introduction](#Introduction)\n",
        "- [Problem Statement](#Problem_Statement)\n",
        "- [Generator](#Generator)\n",
        "- [Models](#Model)\n",
        "    - Conv3D:\n",
        "    -- [Model 1: No of Epochs = 15 , batch_size = 64 ,shape = (120,120) , no of frames = 10](#Model_1)\n",
        "    -- [Model 2: No of Epochs = 20 , batch_size = 20 ,shape = (50,50) , no of frames = 6](#Model_2)\n",
        "    -- [Model 3: No of Epochs = 20 , batch_size = 30 ,shape = (50,50) , no of frames = 10](#Model_3)\n",
        "    -- [Model 4: No of Epochs = 25 , batch_size = 50 ,shape = (120,120) , no of frames = 10](#Model_4)\n",
        "    -- [Model 5: No of Epochs = 25 , batch_size = 50 ,shape = (70,70) , no of frames = 18](#Model_5)\n",
        "    - CNN + RNN : CNN2D LSTM Model - TimeDistributed\n",
        "    -- [Model 6: No of Epochs = 25 , batch_size = 50 ,shape = (70,70), no of frames = 18](#Model_6)\n",
        "    -- [Model 7: No of Epochs = 20 , number of batches=20 ,shape = (50,50), number of frames=10](#Model_7)\n",
        "    - CONV2D + GRU\n",
        "    -- [Model 8: No of frames are 18 , image_height and image_witdth = (50,50) , batch_size 20 , no of epochs = 20](#Model_8)\n",
        "    - Transfer Learning Using MobileNet\n",
        "    -- [Model 9:  No of epochs = 15; batch_size = 5; shape (120,120); no of frames = 18](#Model_9)\n",
        "- [Conclusion](#Conclusion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMtDl6z0CjeN"
      },
      "source": [
        "<h2><a id=\"Introduction\">Introduction</a></h2>\n",
        "\n",
        "In this group project, we are going to build a different model that will be able to predict the 5 gestures correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxQaEA0qCjeO"
      },
      "source": [
        "<h2><a id=\"Problem_Statement\">Problem Statement</a></h2>\n",
        "\n",
        "    - We want to develop a cool feature in the smart-TV that can recognise five different gestures performed by the user which will help users control the TV without using a remote.\n",
        "    - The gestures are continuously monitored by the webcam mounted on the TV. Each gesture corresponds to a specific command:\n",
        "        -- Thumbs up:  Increase the volume\n",
        "        -- Thumbs down: Decrease the volume\n",
        "        -- Left swipe: 'Jump' backwards 10 seconds\n",
        "        -- Right swipe: 'Jump' forward 10 seconds  \n",
        "        -- Stop: Pause the movie"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: numpy in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (1.26.4)\n",
            "Requirement already satisfied: imageio in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (2.37.0)\n",
            "Requirement already satisfied: kagglehub in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (0.3.7)\n",
            "Requirement already satisfied: keras in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (3.3.3)\n",
            "Requirement already satisfied: tensorflow in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (2.16.1)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from imageio) (10.3.0)\n",
            "Requirement already satisfied: model-signing in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from kagglehub) (0.2.0)\n",
            "Requirement already satisfied: packaging in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from kagglehub) (24.0)\n",
            "Requirement already satisfied: requests in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from kagglehub) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from kagglehub) (4.66.4)\n",
            "Requirement already satisfied: absl-py in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from keras) (2.1.0)\n",
            "Requirement already satisfied: rich in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from keras) (13.7.1)\n",
            "Requirement already satisfied: namex in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from keras) (3.11.0)\n",
            "Requirement already satisfied: optree in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from keras) (0.11.0)\n",
            "Requirement already satisfied: ml-dtypes in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from keras) (0.3.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorflow) (4.25.3)\n",
            "Requirement already satisfied: setuptools in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorflow) (65.5.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorflow) (4.11.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.17,>=2.16 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorflow) (2.16.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from requests->kagglehub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from requests->kagglehub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from requests->kagglehub) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from requests->kagglehub) (2024.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: cryptography in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from model-signing->kagglehub) (44.0.0)\n",
            "Requirement already satisfied: in-toto-attestation in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from model-signing->kagglehub) (0.9.3)\n",
            "Requirement already satisfied: sigstore in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from model-signing->kagglehub) (3.6.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from rich->keras) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: cffi>=1.12 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from cryptography->model-signing->kagglehub) (1.17.1)\n",
            "Requirement already satisfied: id>=1.1.0 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from sigstore->model-signing->kagglehub) (1.5.0)\n",
            "Requirement already satisfied: pyasn1~=0.6 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from sigstore->model-signing->kagglehub) (0.6.1)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from sigstore->model-signing->kagglehub) (2.7.4)\n",
            "Requirement already satisfied: pyjwt>=2.1 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from sigstore->model-signing->kagglehub) (2.10.1)\n",
            "Requirement already satisfied: pyOpenSSL>=23.0.0 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from sigstore->model-signing->kagglehub) (25.0.0)\n",
            "Requirement already satisfied: rfc8785~=0.1.2 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from sigstore->model-signing->kagglehub) (0.1.4)\n",
            "Requirement already satisfied: rfc3161-client~=0.1.2 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from sigstore->model-signing->kagglehub) (0.1.2)\n",
            "Requirement already satisfied: sigstore-protobuf-specs==0.3.2 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from sigstore->model-signing->kagglehub) (0.3.2)\n",
            "Requirement already satisfied: sigstore-rekor-types==0.0.18 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from sigstore->model-signing->kagglehub) (0.0.18)\n",
            "Requirement already satisfied: tuf~=5.0 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from sigstore->model-signing->kagglehub) (5.1.0)\n",
            "Requirement already satisfied: platformdirs~=4.2 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from sigstore->model-signing->kagglehub) (4.3.6)\n",
            "Requirement already satisfied: betterproto==2.0.0b6 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from sigstore-protobuf-specs==0.3.2->sigstore->model-signing->kagglehub) (2.0.0b6)\n",
            "Requirement already satisfied: grpclib<0.5.0,>=0.4.1 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from betterproto==2.0.0b6->sigstore-protobuf-specs==0.3.2->sigstore->model-signing->kagglehub) (0.4.7)\n",
            "Requirement already satisfied: python-dateutil<3.0,>=2.8 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from betterproto==2.0.0b6->sigstore-protobuf-specs==0.3.2->sigstore->model-signing->kagglehub) (2.9.0.post0)\n",
            "Requirement already satisfied: pycparser in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from cffi>=1.12->cryptography->model-signing->kagglehub) (2.22)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from pydantic<3,>=2->sigstore->model-signing->kagglehub) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from pydantic<3,>=2->sigstore->model-signing->kagglehub) (2.18.4)\n",
            "Requirement already satisfied: securesystemslib~=1.0 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from tuf~=5.0->sigstore->model-signing->kagglehub) (1.2.0)\n",
            "Requirement already satisfied: email-validator>=2.0.0 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from pydantic[email]<3,>=2->sigstore-rekor-types==0.0.18->sigstore->model-signing->kagglehub) (2.2.0)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from email-validator>=2.0.0->pydantic[email]<3,>=2->sigstore-rekor-types==0.0.18->sigstore->model-signing->kagglehub) (2.6.1)\n",
            "Requirement already satisfied: h2<5,>=3.1.0 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from grpclib<0.5.0,>=0.4.1->betterproto==2.0.0b6->sigstore-protobuf-specs==0.3.2->sigstore->model-signing->kagglehub) (4.2.0)\n",
            "Requirement already satisfied: multidict in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from grpclib<0.5.0,>=0.4.1->betterproto==2.0.0b6->sigstore-protobuf-specs==0.3.2->sigstore->model-signing->kagglehub) (6.0.5)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from h2<5,>=3.1.0->grpclib<0.5.0,>=0.4.1->betterproto==2.0.0b6->sigstore-protobuf-specs==0.3.2->sigstore->model-signing->kagglehub) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /Users/sreedhar.k/.pyenv/versions/3.11.8/lib/python3.11/site-packages (from h2<5,>=3.1.0->grpclib<0.5.0,>=0.4.1->betterproto==2.0.0b6->sigstore-protobuf-specs==0.3.2->sigstore->model-signing->kagglehub) (4.1.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy imageio kagglehub keras tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "s9p-NEyACjeO"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'kagglehub'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkagglehub\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'kagglehub'"
          ]
        }
      ],
      "source": [
        "# Import the following libraries to get started.\n",
        "import numpy as np\n",
        "import os\n",
        "#from scipy.misc import imread, imresize\n",
        "import imageio\n",
        "from PIL import Image\n",
        "import datetime\n",
        "import kagglehub\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bZIPF8dgCpik",
        "outputId": "77d1761b-da13-4539-8a10-4a48c3e27173"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.6), please consider upgrading to the latest version (0.3.7).\n",
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/imsparsh/gesture-recognition?dataset_version_number=2...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1.60G/1.60G [00:23<00:00, 72.3MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/imsparsh/gesture-recognition/versions/2\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"imsparsh/gesture-recognition\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klEjdgGlCjeQ"
      },
      "source": [
        "We set the random seed so that the results don't vary drastically."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qiSahkPxCjeQ"
      },
      "outputs": [],
      "source": [
        "np.random.seed(30)\n",
        "import random as rn\n",
        "rn.seed(30)\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGLQfNIYDu-g",
        "outputId": "1b13c4bb-0af4-4dca-b14d-786a04cbb38e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LICENSE  README.md  train  train.csv  val  val.csv\n"
          ]
        }
      ],
      "source": [
        "!ls /root/.cache/kagglehub/datasets/imsparsh/gesture-recognition/versions/2\n",
        "!cd\n",
        "\n",
        "!chmod 777 /root/.cache/kagglehub/datasets/imsparsh/gesture-recognition/versions/2/train.csv\n",
        "!chmod 777 /root/.cache/kagglehub/datasets/imsparsh/gesture-recognition/versions/2/val.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6MD9uwLCjeQ"
      },
      "source": [
        "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "xm3PVVTlCjeQ"
      },
      "outputs": [],
      "source": [
        "train_doc = np.random.permutation(open('/root/.cache/kagglehub/datasets/imsparsh/gesture-recognition/versions/2/train.csv').readlines())\n",
        "val_doc = np.random.permutation(open('/root/.cache/kagglehub/datasets/imsparsh/gesture-recognition/versions/2/val.csv').readlines())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nLxFcqYCjeR"
      },
      "source": [
        "<h2><a id=\"Generator\">Generator</a></h2>\n",
        "\n",
        "This is one of the most important parts of the code. In the generator, we are going to pre-process the images as we have images of different dimensions (50 x 50, 70 x 70 and 120 x 120) as well as create a batch of video frames. The generator should be able to take a batch of videos as input without any error. Steps like cropping/resizing and normalization should be performed successfully.  We have to experiment with `img_idx`, `y`,`z` and normalization such that we get high accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CYyxFezsCjeR"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "#!pip install scikit-image\n",
        "from skimage.transform import resize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "FkLE6qSbCjeR"
      },
      "outputs": [],
      "source": [
        "def generator(source_path, folder_list, batch_size):\n",
        "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
        "    #img_idx = #create a list of image numbers you want to use for a particular video\n",
        "    while True:\n",
        "        #Shuffle the list of the folders in csv\n",
        "        t = np.random.permutation(folder_list)\n",
        "         #Exact batches of the batch size\n",
        "        num_batches = int(len(t)/batch_size)\n",
        "         #Left over batches which should be handled separately\n",
        "        leftover_batches = len(t) - num_batches * batch_size\n",
        "\n",
        "        for batch in range(num_batches): # we iterate over the number of batches\n",
        "            batch_data = np.zeros((batch_size,len(img_idx),shape_h, shape_w,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
        "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
        "            for folder in range(batch_size): # iterate over the batch_size\n",
        "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
        "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
        "                    image = imageio.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
        "\n",
        "                    #crop the images and resize them. Note that the images are of 2 different shape\n",
        "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
        "\n",
        "                    image = resize(image, (shape_h,shape_w))\n",
        "                    batch_data[folder,idx,:,:,0] = (image[:,:,0]) - 104\n",
        "                    batch_data[folder,idx,:,:,1] = (image[:,:,1]) - 117\n",
        "                    batch_data[folder,idx,:,:,2] = (image[:,:,2]) - 123\n",
        "\n",
        "                #Fill the one hot encoding stuff where we maintain the label\n",
        "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
        "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
        "\n",
        "\n",
        "        # write the code for the remaining data points which are left after full batches\n",
        "        if leftover_batches != 0:\n",
        "            for batch in range(num_batches):\n",
        "                # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
        "                batch_data = np.zeros((batch_size,len(img_idx),shape_h, shape_w,3))\n",
        "                # batch_labels is the one hot representation of the output: 10 videos with 5 columns as classes\n",
        "                batch_labels = np.zeros((batch_size,5))\n",
        "                for folder in range(batch_size): # iterate over the batch_size\n",
        "                    imgs = os.listdir(source_path +'/'+t[batch * batch_size + folder].split(';')[0])\n",
        "                    for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
        "\n",
        "                        image = imageio.imread(source_path +'/'+t[batch * batch_size + folder].split(';')[0] +'/'+imgs[item]).astype(np.float32)\n",
        "                        image = resize(image, (shape_h,shape_w))\n",
        "\n",
        "                        batch_data[folder,idx,:,:,0] = (image[:,:,0]) - 104\n",
        "                        batch_data[folder,idx,:,:,1] = (image[:,:,1]) - 117\n",
        "                        batch_data[folder,idx,:,:,2] = (image[:,:,2]) - 123\n",
        "\n",
        "                    #Fill the one hot encoding stuff where we maintain the label\n",
        "                    batch_labels[folder, int(t[batch * batch_size + folder].split(';')[2])] = 1\n",
        "                yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HExQRYP8CjeR"
      },
      "source": [
        "A video is represented above in the generator as (number of images, height, width, number of channels). We take this into consideration while creating the model architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7xVrrGmCjeR",
        "outputId": "2e330c4f-a5e9-4645-a164-11b2aacefe03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# training sequences = 663\n",
            "# validation sequences = 100\n"
          ]
        }
      ],
      "source": [
        "curr_dt_time = datetime.datetime.now()\n",
        "train_path = '/root/.cache/kagglehub/datasets/imsparsh/gesture-recognition/versions/2/train'\n",
        "val_path = '/root/.cache/kagglehub/datasets/imsparsh/gesture-recognition/versions/2/val'\n",
        "\n",
        "num_train_sequences = len(train_doc)\n",
        "print('# training sequences =', num_train_sequences)\n",
        "num_val_sequences = len(val_doc)\n",
        "print('# validation sequences =', num_val_sequences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxQNS5ieCjeS"
      },
      "source": [
        "<h2><a id=\"Model\">Model</a></h2>\n",
        "\n",
        "Here we make the model using different functionalities that Keras provides. We must use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. We would also use `TimeDistributed` while building a Conv2D + RNN model. Also, the last layer is the softmax. We design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "wYBMQ5EYCjeS"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, GRU, Flatten, TimeDistributed, Flatten, BatchNormalization, Activation,  Dropout, LSTM, ConvLSTM2D\n",
        "from tensorflow.keras import regularizers\n",
        "from keras.layers import Conv3D, MaxPooling3D\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "from keras import optimizers\n",
        "\n",
        "\n",
        "#write your model here\n",
        "class Conv3DModel():\n",
        "\n",
        "    def Model3D(self,frames_to_sample,image_height,image_width):\n",
        "\n",
        "        model = Sequential()\n",
        "        model.add(Conv3D(64, (3,3,3), strides=(1,1,1), padding='same', input_shape=(frames_to_sample,image_height,image_width,3)))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Activation('elu'))\n",
        "        model.add(MaxPooling3D(pool_size=(2,2,1), strides=(2,2,1)))\n",
        "\n",
        "        model.add(Conv3D(128, (3,3,3), strides=(1,1,1), padding='same'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Activation('elu'))\n",
        "        model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2), padding='same'))\n",
        "\n",
        "        # model.add(Dropout(0.25))\n",
        "\n",
        "        model.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Activation('elu'))\n",
        "        model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2), padding='same'))\n",
        "\n",
        "        # model.add(Dropout(0.25))\n",
        "\n",
        "        model.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Activation('elu'))\n",
        "        model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2), padding='same'))\n",
        "\n",
        "        model.add(Flatten())\n",
        "\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(512, activation='elu'))\n",
        "        model.add(Dropout(0.5))\n",
        "        model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "        #write your optimizer TRY OUT WITH ADAM AND SGD\n",
        "        '''\n",
        "        Classes\n",
        "        class Adadelta: Optimizer that implements the Adadelta algorithm.\n",
        "\n",
        "        class Adagrad: Optimizer that implements the Adagrad algorithm.\n",
        "\n",
        "        class Adam: Optimizer that implements the Adam algorithm.\n",
        "\n",
        "        class Adamax: Optimizer that implements the Adamax algorithm.\n",
        "\n",
        "        class Ftrl: Optimizer that implements the FTRL algorithm.\n",
        "\n",
        "        class Nadam: Optimizer that implements the NAdam algorithm.\n",
        "\n",
        "        class Optimizer: Base class for Keras optimizers.\n",
        "\n",
        "        class RMSprop: Optimizer that implements the RMSprop algorithm.\n",
        "\n",
        "        class SGD: Gradient descent (with momentum) optimizer.\n",
        "        '''\n",
        "\n",
        "        optimiser = tf.keras.optimizers.SGD(learning_rate=0.001, decay=1e-6, momentum=0.7, nesterov=True)\n",
        "        model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "        return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hNdIHyFCjeT"
      },
      "source": [
        "Once we have written the model, the next step is to `compile` the model. When we print the `summary` of the model, we can see the total number of parameters we have to train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Fkd3w6IcCjeT"
      },
      "outputs": [],
      "source": [
        "#Global vars\n",
        "def global_vars(img_idx,shape_h,shape_w,batch_size,num_epochs):\n",
        "    print(\"the number of images we will be feeding in the input for a video {}\".format(len(img_idx)))\n",
        "    return img_idx,shape_h,shape_w,batch_size,num_epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f2O2mRJCjeT"
      },
      "source": [
        "<h2><a id=\"Model_1\">Model 1:</a></h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sUnqIiyECjeT",
        "outputId": "5e1f78ca-28f9-42ab-8a04-b1cda3b78b01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the number of images we will be feeding in the input for a video 10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:86: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv3d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │           <span style=\"color: #00af00; text-decoration-color: #00af00\">5,248</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling3d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)      │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv3d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">221,312</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">120</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)     │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling3d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv3d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">884,992</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_2                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling3d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv3d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv3D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,769,728</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_3                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling3d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling3D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30720</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30720</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │      <span style=\"color: #00af00; text-decoration-color: #00af00\">15,729,152</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,565</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv3d (\u001b[38;5;33mConv3D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │           \u001b[38;5;34m5,248\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │             \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation (\u001b[38;5;33mActivation\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling3d (\u001b[38;5;33mMaxPooling3D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m64\u001b[0m)      │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv3d_1 (\u001b[38;5;33mConv3D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m128\u001b[0m)     │         \u001b[38;5;34m221,312\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m128\u001b[0m)     │             \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_1 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m120\u001b[0m, \u001b[38;5;34m128\u001b[0m)     │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling3d_1 (\u001b[38;5;33mMaxPooling3D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv3d_2 (\u001b[38;5;33mConv3D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │         \u001b[38;5;34m884,992\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_2                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │           \u001b[38;5;34m1,024\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_2 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling3d_2 (\u001b[38;5;33mMaxPooling3D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv3d_3 (\u001b[38;5;33mConv3D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │       \u001b[38;5;34m1,769,728\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ batch_normalization_3                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │           \u001b[38;5;34m1,024\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ activation_3 (\u001b[38;5;33mActivation\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling3d_3 (\u001b[38;5;33mMaxPooling3D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30720\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30720\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │      \u001b[38;5;34m15,729,152\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)                   │           \u001b[38;5;34m2,565\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,615,813</span> (71.01 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m18,615,813\u001b[0m (71.01 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,614,405</span> (71.01 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m18,614,405\u001b[0m (71.01 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,408</span> (5.50 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m1,408\u001b[0m (5.50 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Model 1: No of Epochs = 15 , batch_size = 64 ,shape = (120,120) , no of frames = 10\n",
        "\n",
        "img_idx,shape_h,shape_w,batch_size,num_epochs = global_vars([6,8,10,12,14,16,20,22,24,26],120,120,64,15)\n",
        "conv_model1=Conv3DModel()\n",
        "conv_model1=conv_model1.Model3D(frames_to_sample=len(img_idx),image_height=shape_h,image_width=shape_w)\n",
        "conv_model1.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEtTHWV9CjeU"
      },
      "source": [
        "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cRogtFepCjeU"
      },
      "outputs": [],
      "source": [
        "train_generator = generator(train_path, train_doc, batch_size)\n",
        "val_generator = generator(val_path, val_doc, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "wa0DEMb4CjeU"
      },
      "outputs": [],
      "source": [
        "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
        "\n",
        "if not os.path.exists(model_name):\n",
        "    os.mkdir(model_name)\n",
        "\n",
        "#Fix the file path\n",
        "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "#Callback to save the Keras model or model weights at some frequency.\n",
        "#ModelCheckpoint callback is used in conjunction with training using model.fit() to save a model or weights.\n",
        "#path to save the model file.\n",
        "#\"val_loss\" to monitor the model's total loss in validation.\n",
        "#saves when the model is considered the \"best\"\n",
        "#the model's weights will be saved\n",
        "#the minimization of the monitored quantity\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto',save_freq='epoch')\n",
        "\n",
        "#Reduce learning rate when a metric has stopped improving.\n",
        "#LR = ReduceLROnPlateau(monitor, factor, aptience, min_lr)\n",
        "#monitor: quantity to be monitored.\n",
        "#factor: factor by which the learning rate will be reduced. new_lr = lr * factor.\n",
        "#patience: number of epochs with no improvement after which learning rate will be reduced.\n",
        "#min_lr: lower bound on the learning rate.\n",
        "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1, mode='min', epsilon=0.0001, cooldown=0, min_lr=0.00001)\n",
        "\n",
        "EarlyStop = EarlyStopping(monitor='val_loss', patience=6 )\n",
        "# write the REducelronplateau code here\n",
        "callbacks_list = [checkpoint, LR]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qG7T9VHVCjeU"
      },
      "source": [
        "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "f89baIs7CjeU"
      },
      "outputs": [],
      "source": [
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XgxdMWbmCjeU",
        "outputId": "5a85bfbe-3e1f-4750-b518-edeac1c5a067"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11\n",
            "2\n"
          ]
        }
      ],
      "source": [
        "print(steps_per_epoch)\n",
        "print(validation_steps)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2h2BEFZUCjeV"
      },
      "source": [
        "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "qEemFlCECjeV",
        "outputId": "89228e5f-a65f-4751-9057-db73b7582a78"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-7-6332446c20a2>:18: DeprecationWarning: Starting with ImageIO v3 the behavior of this function will switch to that of iio.v3.imread. To keep the current behavior (and make this warning disappear) use `import imageio.v2 as imageio` or call `imageio.v2.imread` directly.\n",
            "  image = imageio.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source path =  /root/.cache/kagglehub/datasets/imsparsh/gesture-recognition/versions/2/train ; batch size = 64\n",
            "Epoch 1/15\n"
          ]
        }
      ],
      "source": [
        "conv_model1.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
        "                     callbacks=callbacks_list, validation_data=val_generator,\n",
        "                     validation_steps=validation_steps, class_weight=None, initial_epoch=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4ipi4mXCjeV"
      },
      "source": [
        "#### Insights:\n",
        "    Model 1 is giving the out of memory error with batch size 64. We try with less batch size and shapes to further improve the performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8RVeCZRCjeV"
      },
      "source": [
        "<h2><a id=\"Model_2\">Model 2:</a></h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TEDAD0bXCjeV"
      },
      "outputs": [],
      "source": [
        "# Model 2: No of Epochs = 20; batch_size = 20; shape = (50,50); no of frames = 6\n",
        "\n",
        "img_idx,shape_h,shape_w,batch_size,num_epochs = global_vars(list(range(0,30,5)),50,50,20,20)\n",
        "conv_model2=Conv3DModel()\n",
        "conv_model2=conv_model2.Model3D(frames_to_sample=len(img_idx),image_height=shape_h,image_width=shape_w)\n",
        "conv_model2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4-omQgSLCjeV"
      },
      "outputs": [],
      "source": [
        "train_generator = generator(train_path, train_doc, batch_size)\n",
        "val_generator = generator(val_path, val_doc, batch_size)\n",
        "\n",
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1\n",
        "\n",
        "print(steps_per_epoch)\n",
        "print(validation_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dTbir1wFCjeV"
      },
      "outputs": [],
      "source": [
        "conv_model2.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
        "                    callbacks=callbacks_list, validation_data=val_generator,\n",
        "                    validation_steps=validation_steps, class_weight=None, initial_epoch=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyuXZz8oCjeW"
      },
      "source": [
        "#### Insights:\n",
        "    - Number of Epochs =20; Batch size=20; Number of frames=6\n",
        "    - Taking the Frames with the step size 5 and taking 6 frames with shape (50,50) have increased the performance tremendously for both the training and validation set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX9yXSNqCjeW"
      },
      "source": [
        "<h2><a id=\"Model_3\">Model 3: </a></h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FhLTC4keCjeW",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#No of Epochs = 20; batch_size = 30; shape = (50,50); no of frames = 10\n",
        "img_idx,shape_h,shape_w,batch_size,num_epochs = global_vars(list(range(0,30,3)),50,50,20,20)\n",
        "conv_model3=Conv3DModel()\n",
        "conv_model3=conv_model3.Model3D(frames_to_sample=len(img_idx),image_height=shape_h,image_width=shape_w)\n",
        "conv_model3.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "9fS1lE3OCjeW"
      },
      "outputs": [],
      "source": [
        "train_generator = generator(train_path, train_doc, batch_size)\n",
        "val_generator = generator(val_path, val_doc, batch_size)\n",
        "\n",
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1\n",
        "\n",
        "print(steps_per_epoch)\n",
        "print(validation_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5XfKSWE-CjeW"
      },
      "outputs": [],
      "source": [
        "conv_model3.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
        "                    callbacks=callbacks_list, validation_data=val_generator,\n",
        "                    validation_steps=validation_steps, class_weight=None, initial_epoch=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Frn4YXZZCjeW"
      },
      "source": [
        "#### Insights:\n",
        "    Model 3: Number of Epochs =20; Batch size=30; shape = (50,50); Number of frames=10\n",
        "    Keeping the same shape and increasing the number of frames we have observed that Validation Accuracy decreased and slightly seems to be overfitting as compared to Model-2\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHh7xJfXCjec"
      },
      "source": [
        "<h2><a id=\"Model_4\">Model 4: </a></h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "OB_dhBSBCjec"
      },
      "outputs": [],
      "source": [
        "#No of Epochs = 25 , batch_size = 50 ,shape = (100,100) , no of frames = 10\n",
        "img_idx,shape_h,shape_w,batch_size,num_epochs = global_vars(list(range(5,28,2)),100,100,50,25)\n",
        "conv_model4=Conv3DModel()\n",
        "conv_model4=conv_model4.Model3D(frames_to_sample=len(img_idx),image_height=shape_h,image_width=shape_w)\n",
        "conv_model4.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "4u_77OgaCjec"
      },
      "outputs": [],
      "source": [
        "train_generator = generator(train_path, train_doc, batch_size)\n",
        "val_generator = generator(val_path, val_doc, batch_size)\n",
        "\n",
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1\n",
        "\n",
        "print(steps_per_epoch)\n",
        "print(validation_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "J34CPv05Cjed"
      },
      "outputs": [],
      "source": [
        "conv_model4.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
        "                     callbacks=callbacks_list, validation_data=val_generator,\n",
        "                     validation_steps=validation_steps, class_weight=None, initial_epoch=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LMuM8HfCjed"
      },
      "source": [
        "#### Insights:\n",
        "Model 4: This model seems to be overfitting. Increasing the image size decreases the accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSCvXvoxCjed"
      },
      "source": [
        "<h2><a id=\"Model_5\">Model 5: </a></h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "GHdu8FLACjed"
      },
      "outputs": [],
      "source": [
        "#No of Epochs = 25 , batch_size = 50 ,shape = (70,70) , no of frames = 18\n",
        "img_idx,shape_h,shape_w,batch_size,num_epochs = global_vars([0,1,2,4,6,8,10,12,14,16,18,20,22,24,26,27,28,29],70,70,50,34)\n",
        "conv_model5=Conv3DModel()\n",
        "conv_model5=conv_model5.Model3D(frames_to_sample=len(img_idx),image_height=shape_h,image_width=shape_w)\n",
        "conv_model5.summary()\n",
        "\n",
        "train_generator = generator(train_path, train_doc, batch_size)\n",
        "val_generator = generator(val_path, val_doc, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "djxaKX9nCjed"
      },
      "outputs": [],
      "source": [
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hO5W7WloCjed"
      },
      "outputs": [],
      "source": [
        "print(steps_per_epoch)\n",
        "print(validation_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TFjpBxYQCjed"
      },
      "outputs": [],
      "source": [
        "conv_model5.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
        "                    callbacks=callbacks_list, validation_data=val_generator,\n",
        "                    validation_steps=validation_steps, class_weight=None, initial_epoch=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsKgKtS8Cjed"
      },
      "source": [
        "#### Insights:\n",
        "    Model 5 is clearly an overfit model can see that increasing in number of frames and epochs causing the noise to be learned also from all the frames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7ejk0qECjee"
      },
      "source": [
        "#### Overall Insights for Model 1 to 5:\n",
        "    Based on our experiment the final model will be model 2 - Less no of frames and reducing image size to 50,50 giving good results\n",
        "    Model 2 No of Epochs = 20 , batch_size = 20 ,shape = (50,50) , no of frames = 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z13vNL3UCjee"
      },
      "source": [
        "<h2><a id=\"Model_6\">Model 6 <br></a></h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "U68RBnObCjee"
      },
      "outputs": [],
      "source": [
        "#Taking image_height and image_width as 70,70 , batch size 50 and no of epochs 25\n",
        "#Switching Model architecture to Conv2D+LSTM\n",
        "# Conv2D_18, 70, 70, 16\n",
        "# LSTM_512\n",
        "# Dense_512_5\n",
        "\n",
        "from keras.layers.convolutional import  Conv2D, MaxPooling2D\n",
        "from keras.layers import TimeDistributed,LSTM ,ConvLSTM2D\n",
        "model = Sequential([\n",
        "    TimeDistributed(Conv2D(16, (3,3), padding='same', activation='relu'), input_shape=(len(img_idx),shape_h,shape_w,3)),\n",
        "    TimeDistributed(BatchNormalization()),\n",
        "    TimeDistributed(MaxPooling2D((2,2))),\n",
        "\n",
        "    TimeDistributed(Conv2D(32, (3,3), padding='same', activation='relu')),\n",
        "    TimeDistributed(BatchNormalization()),\n",
        "    TimeDistributed(MaxPooling2D((2,2))),\n",
        "\n",
        "    TimeDistributed(Conv2D(64, (3,3), padding='same', activation='relu')),\n",
        "    TimeDistributed(BatchNormalization()),\n",
        "    TimeDistributed(MaxPooling2D((2,2))),\n",
        "\n",
        "    TimeDistributed(Conv2D(128, (3,3), padding='same', activation='relu')),\n",
        "    TimeDistributed(BatchNormalization()),\n",
        "    TimeDistributed(MaxPooling2D((2,2))),\n",
        "\n",
        "    TimeDistributed(Conv2D(256, (3,3), padding='same', activation='relu')),\n",
        "    TimeDistributed(BatchNormalization()),\n",
        "    TimeDistributed(MaxPooling2D((2,2))),\n",
        "\n",
        "    TimeDistributed(Flatten()),\n",
        "    LSTM(512),\n",
        "    Dropout(0.2),\n",
        "\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "\n",
        "    Dense(5, activation='softmax')\n",
        "], name=\"conv_2d_lstm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "v8epa-GpCjee"
      },
      "outputs": [],
      "source": [
        "optimiser = tf.keras.optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.7, nesterov=True)\n",
        "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "sIBifCJSCjee"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "SUL5ZgymCjee"
      },
      "outputs": [],
      "source": [
        "train_generator = generator(train_path, train_doc, 20)\n",
        "val_generator = generator(val_path, val_doc, 20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_TS_nHiaCjee"
      },
      "outputs": [],
      "source": [
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "T8hiftpMCjee"
      },
      "outputs": [],
      "source": [
        "print(steps_per_epoch)\n",
        "print(validation_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "exKDYVNlCjef"
      },
      "outputs": [],
      "source": [
        "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1,\n",
        "                    callbacks=callbacks_list, validation_data=val_generator,\n",
        "                    validation_steps=validation_steps, class_weight=None, initial_epoch=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cM6rS1YdCjef"
      },
      "source": [
        "#### Insights:\n",
        "    Model-6 is clearly overfitting.\n",
        "    We will change the number of frames, image size and check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DSY3uax6Cjef"
      },
      "source": [
        "<h2><a id=\"Model_7\">Model 7:</a></h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llpZ-I6wCjef"
      },
      "source": [
        "    No of Epochs = 20 , number of batches=20 ,shape = (50,50), number of frames=10\n",
        "    img_idx,shape_h,shape_w,batch_size,num_epochs = global_vars(list(range(0,30,3)),50,50,20,20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4sLxQ1hCjef"
      },
      "source": [
        "The number of images we will be feeding in the input for a video 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "IVBsOreECjef"
      },
      "outputs": [],
      "source": [
        "#Switching Model architecture to Conv2D+LSTM\n",
        "\n",
        "from keras.layers.convolutional import  Conv2D, MaxPooling2D\n",
        "from keras.layers import TimeDistributed,LSTM ,ConvLSTM2D\n",
        "model = Sequential([\n",
        "    TimeDistributed(Conv2D(16, (3,3), padding='same', activation='relu'), input_shape=(len(img_idx),shape_h,shape_w,3)),\n",
        "    TimeDistributed(BatchNormalization()),\n",
        "    TimeDistributed(MaxPooling2D((2,2))),\n",
        "\n",
        "    TimeDistributed(Conv2D(32, (3,3), padding='same', activation='relu')),\n",
        "    TimeDistributed(BatchNormalization()),\n",
        "    TimeDistributed(MaxPooling2D((2,2))),\n",
        "\n",
        "    TimeDistributed(Conv2D(64, (3,3), padding='same', activation='relu')),\n",
        "    TimeDistributed(BatchNormalization()),\n",
        "    TimeDistributed(MaxPooling2D((2,2))),\n",
        "\n",
        "    TimeDistributed(Conv2D(128, (3,3), padding='same', activation='relu')),\n",
        "    TimeDistributed(BatchNormalization()),\n",
        "    TimeDistributed(MaxPooling2D((2,2))),\n",
        "\n",
        "    TimeDistributed(Conv2D(256, (3,3), padding='same', activation='relu')),\n",
        "    TimeDistributed(BatchNormalization()),\n",
        "    TimeDistributed(MaxPooling2D((2,2))),\n",
        "\n",
        "    TimeDistributed(Flatten()),\n",
        "    LSTM(512),\n",
        "    Dropout(0.2),\n",
        "\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "\n",
        "    Dense(5, activation='softmax')\n",
        "], name=\"conv_2d_lstm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NxrmHKBoCjef"
      },
      "outputs": [],
      "source": [
        "optimiser = tf.keras.optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.7, nesterov=True)\n",
        "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "DCYw21l7Cjef"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5w4hI_VoCjeg"
      },
      "outputs": [],
      "source": [
        "train_generator = generator(train_path, train_doc, batch_size)\n",
        "val_generator = generator(val_path, val_doc, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jaN1fOMBCjeg"
      },
      "outputs": [],
      "source": [
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NQOqiAbvCjeg"
      },
      "outputs": [],
      "source": [
        "print(steps_per_epoch)\n",
        "print(validation_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "u8Cx18d5Cjeg"
      },
      "outputs": [],
      "source": [
        "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs,verbose=1,\n",
        "                    callbacks=callbacks_list, validation_data=val_generator,\n",
        "                    validation_steps=validation_steps, class_weight=None, initial_epoch=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DW1eJChuCjeg"
      },
      "source": [
        "#### Insights:\n",
        "    Model 7 is also clearly overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3SwSnsUCjeg"
      },
      "source": [
        "<h2><a id=\"Model_8\">Model 8: </a></h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTWjjyE8Cjeh"
      },
      "source": [
        "CONV2D + GRU Changed the no of layers , no of frames are 18 , image_height and image_witdth = (50,50) , batch_size 20 , no of epochs = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XVXAzEkHCjeh"
      },
      "outputs": [],
      "source": [
        "img_idx,shape_h,shape_w,batch_size,num_epochs = global_vars([0,1,2,4,6,8,10,12,14,16,18,20,22,24,26,27,28,29],50,50,20,20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "mjTsslXjCjeh"
      },
      "outputs": [],
      "source": [
        "from keras.layers.convolutional import  Conv2D, MaxPooling2D\n",
        "from keras.layers import TimeDistributed,LSTM ,ConvLSTM2D\n",
        "model = Sequential()\n",
        "model.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu'),\n",
        "                                  input_shape=(len(img_idx),shape_h,shape_w,3)))\n",
        "model.add(TimeDistributed(BatchNormalization()))\n",
        "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "\n",
        "model.add(TimeDistributed(Conv2D(32, (3, 3) , padding='same', activation='relu')))\n",
        "model.add(TimeDistributed(BatchNormalization()))\n",
        "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "\n",
        "model.add(TimeDistributed(Conv2D(64, (3, 3) , padding='same', activation='relu')))\n",
        "model.add(TimeDistributed(BatchNormalization()))\n",
        "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "\n",
        "model.add(TimeDistributed(Conv2D(128, (3, 3) , padding='same', activation='relu')))\n",
        "model.add(TimeDistributed(BatchNormalization()))\n",
        "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "\n",
        "\n",
        "model.add(TimeDistributed(Flatten()))\n",
        "\n",
        "\n",
        "model.add(GRU(64))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Dense(64,activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Dense(5, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "zlHWk6G3Cjeh"
      },
      "outputs": [],
      "source": [
        "optimiser = tf.keras.optimizers.SGD(lr=0.001, decay=1e-6, momentum=0.7, nesterov=True)\n",
        "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "C4eeJnbECjeh"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-Mcau27tCjeh"
      },
      "outputs": [],
      "source": [
        "train_generator = generator(train_path, train_doc, batch_size)\n",
        "val_generator = generator(val_path, val_doc, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "TLGeuJ3fCjeh"
      },
      "outputs": [],
      "source": [
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5MyOegnWCjei"
      },
      "outputs": [],
      "source": [
        "print(steps_per_epoch)\n",
        "print(validation_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WZVsI6-JCjei"
      },
      "outputs": [],
      "source": [
        "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs,verbose=1,\n",
        "                    callbacks=callbacks_list, validation_data=val_generator,\n",
        "                    validation_steps=validation_steps, class_weight=None, initial_epoch=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sp3yrKVRCjei"
      },
      "source": [
        "#### Insights:\n",
        "    Model 8 is overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYAYauX3Cjei"
      },
      "source": [
        "<h2><a id=\"Model_9\">Model 9 Using Transfer Learning - MobileNet</a></h2>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Inl4yylWCjei"
      },
      "outputs": [],
      "source": [
        "img_idx,shape_h,shape_w,batch_size,num_epochs = global_vars([0,1,2,4,6,8,10,12,14,16,18,20,22,24,26,27,28,29],120,120,5,15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "WGL76df2Cjei"
      },
      "outputs": [],
      "source": [
        "from keras.layers.convolutional import  Conv2D, MaxPooling2D\n",
        "from keras.layers import TimeDistributed,LSTM ,ConvLSTM2D\n",
        "from keras.applications import mobilenet\n",
        "mobilenet_transfer = mobilenet.MobileNet(weights='imagenet', include_top=False)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(TimeDistributed(mobilenet_transfer,input_shape=(len(img_idx),shape_h,shape_w,3)))\n",
        "\n",
        "model.add(TimeDistributed(BatchNormalization()))\n",
        "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "model.add(TimeDistributed(Flatten()))\n",
        "\n",
        "\n",
        "model.add(GRU(128))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Dense(128,activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Dense(5, activation='softmax'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "NBM0iSgpCjei"
      },
      "outputs": [],
      "source": [
        "optimiser = tf.keras.optimizers.Adam()\n",
        "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "smisCv2nCjei"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Rjm56HNuCjej"
      },
      "outputs": [],
      "source": [
        "train_generator = generator(train_path, train_doc, batch_size)\n",
        "val_generator = generator(val_path, val_doc, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jOjBO8jzCjej"
      },
      "outputs": [],
      "source": [
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "orXgZWtWCjej"
      },
      "outputs": [],
      "source": [
        "print(steps_per_epoch)\n",
        "print(validation_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "PFFTOH4kCjej"
      },
      "outputs": [],
      "source": [
        "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs,verbose=1,\n",
        "                    callbacks=callbacks_list, validation_data=val_generator,\n",
        "                    validation_steps=validation_steps, class_weight=None, initial_epoch=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMM9qshoCjej"
      },
      "source": [
        "<h2><a id=\"Conclusion\">Conclusion</a></h2>\n",
        "\n",
        "- # Model Statistics\n",
        "\n",
        "- # Conv3D\n",
        "\n",
        "- Model 1 : No of Epochs = 15 , batch_size = 64 ,shape = (120,120) , no of frames = 10\n",
        "- - - - Model 1 is giving the out of memory error with batch size 64. We try with less batch size and shapes to further improve the performance and accuracy\n",
        "\n",
        "- Model 2 : No of Epochs = 20 , batch_size = 20 ,shape = (50,50) , no of frames = 6\n",
        "\n",
        "- - - - Training Accuracy : 95.74% , Validation Accuracy : 89% ,\n",
        "- - - - Model Analysis : Training and validation Accuracy are good so that we can conclude that with above set of parameters model is giving good results\n",
        "\n",
        "- Model 3 : No of Epochs = 20 , batch_size = 30 ,shape = (50,50) , no of frames = 10\n",
        "\n",
        "- - - - Training Accuracy : 95.29% , Validation Accuracy : 87%\n",
        "- - - - Model Analysis : Keeping the same shape and increasing the number of frames we have observed that validation accuracy decreased and seems to be overfitting as compared to Model-2\n",
        "\n",
        "- Model 4 : No of Epochs = 25 , batch_size = 50 ,shape = (100,100) , no of frames = 10\n",
        "\n",
        "- - - - Training Accuracy : 91.71% , Validation Accuracy : 86%\n",
        "- - - - Model Analysis : Increasing the image size decreases the accuracy. Also, this model seems to be overfitting.\n",
        "\n",
        "- Model 5 : No of Epochs = 25 , Batch_size = 50 , shape = (70,70) , no of frames = 18\n",
        "\n",
        "- - - - Training Accuracy : 95.71% , Validation Accuracy : 87%\n",
        "- - - - Model Analysis : This model is clearly an overfit model can see that increasing in number of frames and epochs causing the noise to be learned also from all the frames\n",
        "\n",
        "- # CNN + RNN : CNN2D LSTM Model - TimeDistributed\n",
        "\n",
        "- Model 6 : No of Epochs = 25 , Batch_size = 50 , shape = (70,70) , no of frames = 18\n",
        "\n",
        "- - - - Training Accuracy : 81.79% , Validation Accuracy : 60%\n",
        "- - - - Model Analysis : This model is clearly Overfitting\n",
        "\n",
        "- Model 7 : No of epochs = 20 , batch_size = 20 , shape  (50,50) , no of frames  = 10\n",
        "\n",
        "- - - - Training Accuracy : 84.71% , Validation Accuracy : 67%\n",
        "- - - - Model Analysis : This model is clearly overfitting\n",
        "\n",
        "- # CONV2D + GRU\n",
        "\n",
        "- Model 8 : No of epochs = 20 , batch_size = 20 , shape  (50,50) , no of frames  = 18\n",
        "\n",
        "- - - - Training Accuracy : 94.26%, Validation Accuracy : 72%\n",
        "- - - - Model Analysis : This model is overfitting\n",
        "\n",
        "- # Transfer Learning Using MobileNet\n",
        "\n",
        "-  Model 9 : No of epochs = 15 , batch_size = 5 , shape  (120,120) , no of frames  = 18\n",
        "\n",
        "- - - - Training Accuracy : 99.55% , Validation Accuracy : 95%\n",
        "- - - - Model Analysis : This is so far the best model that we got with better accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "B_gWdMGLCjej"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
